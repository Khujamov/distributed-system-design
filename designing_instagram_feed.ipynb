{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Designing Instagram Feed\n",
    "\n",
    "A feed is a constantly updating scrollable list of posts, photos, videos, and status updates from all the people and pages a user follows.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Requirements and System Goals\n",
    "\n",
    "### Functional requirements\n",
    "1. Feeds may contain images, videos and text.\n",
    "2. Feeds are generated from the posts belonging to the pages and people the user follows.\n",
    "3. The service should support appending new posts as they arrive to the feed for all active users\n",
    "\n",
    "### Non-functional requirements \n",
    "1. The system should be able to generate a user's newsfeed in real-time - maximum latency seen by the end user should be about 2s."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Capacity Estimation and Constraints\n",
    "Assume on average a user has 300 friends and follows 200 pages.\n",
    "\n",
    "**Traffic Estimates:** A typical user checks their feed about 5 times per day on average. If we have 200 million daily active users, then:\n",
    "\n",
    "```text\n",
    "Per day: 200M * 5 => 1B requests\n",
    "\n",
    "Per second:   1B / 86400 sec => 11500 reqs/sec \n",
    "```\n",
    "\n",
    "**Storage estimates:** Assume we have on average 500 posts for each user's feed that we want to keep in memory for fast fetching. \n",
    "For simplicity, let's also assume an average photo file size posted would be about 200KB. This means we need about 200KB X 500 = 10 MB per user. \n",
    "To store all this data for all active users, we'll need:\n",
    "\n",
    "```text\n",
    "    200 M active users * 10MB  ~= 2 Petabyte of memory\n",
    "```\n",
    "\n",
    "If a server can hold 100 GB memory, we'd need about 20000 machines to keep the top 500 posts in memory.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. System APIs\n",
    "> By defining the system APIs, you are explicitly stating what is expected from the system\n",
    "\n",
    "We'll have REST APIs to expose our service's functionality.\n",
    "\n",
    "**Getting User Feed**\n",
    "\n",
    "```python\n",
    "getUserFeed(\n",
    "    api_dev_key: int,  # Key of a registered user, used to throttle users based on their allocated quota.\n",
    "    user_id: int,  # The Id of the user whom the system will generate the feed.\n",
    "    since_id: int,  # (Optional) Return results with IDs more recent than this ID.\n",
    "    count: int ,   # (Optional) Specifies number of feed items to try and retrieve.\n",
    "    max_id: int,   # (Optional) Returns results with IDs younger than the specified ID.\n",
    "    exclude_replies  # (Optional) Prevents replies from appearing in the results.\n",
    "```\n",
    "\n",
    "**Returns:** (JSON) object containing a list of feed items. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Database Design\n",
    "There are three major objects: User, Entity (Business Accounts, Brands, Pages etc) and Post (A feed item). \n",
    "    * A user follows entities and other users.\n",
    "    * Users and entities can both post a Post which can contain text, images, or videos\n",
    "    * Each Post has a UserID of the user who created it. \n",
    "    * For simplicity, let's assume only users can create a post.\n",
    "    * A Post can optionally have an EntityID that points to the page or business entity where the post was created.\n",
    "\n",
    "If we use a relational DB, we can model two relations: User-to-Entity and Post-to-Media relation. Since each user can be friends with many people and follow a lot of entities, we can store this relation in a separate table. \n",
    "\n",
    "| Users |                      |\n",
    "|:----:|:-----------------------|\n",
    "|PK   |  **UserID: int**         |\n",
    "|     | Name: varchar(32)       |\n",
    "|     | Email: varchar(32)      |\n",
    "|     | DOB: datetime           |\n",
    "|     | CreatedAt: datetime  |\n",
    "|     | LastLogin: datetime     |\n",
    "\n",
    "&nbsp;\n",
    "\n",
    " | Entity |                      |\n",
    "|:----:|:-----------------------|\n",
    "|PK   |  **EntityID: int**         |\n",
    "|     | Name: varchar(32)       |\n",
    "|     | Type: int               |\n",
    "|     | Email: varchar(32)           |\n",
    "|     | Description: varchar(512)      |\n",
    "|     | Phone: varchar(12)           |\n",
    "|     | CreatedAt: datetime  |\n",
    "|     | LastLogin: datetime     |\n",
    "\n",
    "\n",
    "&nbsp;\n",
    "\n",
    "| UserFollow |                      |\n",
    "|:----:|:-----------------------|\n",
    "|PK   |  (**UserID, EntityOrFriendID: int**)          |\n",
    "|     |  Type: enum (0, 1) |\n",
    "\n",
    "The **Type** column above identifies if an entity being followed is a user or an entity.\n",
    "We can also have a table for the Media to Post relation.\n",
    "\n",
    "&nbsp;\n",
    "\n",
    "| Post |                      |\n",
    "|:----:|:-----------------------|\n",
    "|PK   |  **PostID: int**         |\n",
    "|     | UserID: int |\n",
    "|     | Contents: varchar(256)      |\n",
    "|     | EntityID: int           |\n",
    "|     | Latitute: int           |\n",
    "|     | Longitude: int  |\n",
    "|     | CreatedAt: datetime     |\n",
    "|    |  Likes: int         |\n",
    "\n",
    "&nbsp;\n",
    "\n",
    "| Media |                      |\n",
    "|:----:|:-----------------------|\n",
    "|     | MediaID: int |\n",
    "|     | Type: enum |\n",
    "|     | Description: varchar(256)      |\n",
    "|     | Path: int           |\n",
    "|     | Latitute: int           |\n",
    "|     | Longitude: int  |\n",
    "|     | CreatedAt: datetime     |\n",
    "\n",
    "&nbsp;\n",
    "\n",
    "| PostMedia |                      |\n",
    "|:----:|:-----------------------|\n",
    "|PK   |  (**PostID, MediaID: int**)          |\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. High Level System Design\n",
    "At a high level we have two system parts:\n",
    "Feed generation and Feed publishing.\n",
    "\n",
    "#### Feed Generation\n",
    "Whenever our system receives a request to generate a feed for a user, we'll perform these steps:\n",
    "    1. Get all UserIDs and EntityIDs that the user follows\n",
    "    2. Retrieve latest and most popular posts for those IDs\n",
    "    3. Rank them based on relevance to the user. This is the user's current feed.\n",
    "    4. Store the feed in a cache.\n",
    "    5. Return top posts to be rendered on the user's feed.\n",
    "    6. On front-end, when the user reaches the end of the loaded feed, fetch the next posts from the cache server.\n",
    "    7. Periodically rank and add new posts to the user's feed.\n",
    "    8. Notify user that there are new posts\n",
    "    \n",
    "#### Feed Publishing\n",
    "When the user loads her feed, she has request and pull posts from the server. When she reaches the end of her current feed, the server can push new posts.\n",
    "\n",
    "Should the server notify the user then the user can pull, or should the server just push new posts?\n",
    "\n",
    "At a high level, we'll have the following components:\n",
    "\n",
    "1. Web Servers: maintain the connection to the user to allow data transfer between user client and server.\n",
    "2. Application Server: executes the work of storing new posts in the DB servers, as well as retrieval from the DB and pushing the feed to the user.\n",
    "3. Metadata DB and Cache: store metadata about Users, Pages, Businesses, etc\n",
    "4. Post DB and Cache: to store metadata about posts and their contents\n",
    "5. Video/Photo storage and Cache: Blob storage to store all media in the posts\n",
    "6. Feed generation service: to get and rank relevant posts for a user and generate the feed, and store in the cache.\n",
    "7. Feed notification service: to notify user that there are newer feed posts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Detailed Component Design\n",
    "\n",
    "Let's look at generating the feed. The query would look something like this:\n",
    "\n",
    "~~~mysql\n",
    "SELECT PostID FROM Post WHERE UserID IN\n",
    "    (SELECT EntityOrFriendID FROM UserFollow WHERE UserID = <user_id> AND type = 0) -- user\n",
    "    UNION\n",
    "SELECT PostID from Post WHERE EntityID IN\n",
    "    (SELECT EntityID FROM UserFollow WHERE UserID = <user_id> AND type = 1) -- entity\n",
    "ORDER BY CreatedAt DESC \n",
    "LIMIT 100\n",
    "~~~\n",
    "We want to avoid a direct query to the DB due to high latency. \n",
    "\n",
    "We also want to avoid generating the feed when a user loads the page because it will be slow and have a high latency.\n",
    "\n",
    "Also, the server notifying about new posts to user with lots of followers could lead to heavy loads. To improve on all this, we can pre-generate the feed and store it in memory.\n",
    "\n",
    "### Offline generation \n",
    "We can have severs dedicated to continuously generate feeds and store in memory. When a user requests for the new posts, we can simply serve it from the stored location. Therefore a user's feed is compiled not on load but on a regular basis and returned to users whenever they request it.\n",
    "\n",
    "\n",
    "When the servers need to generate feed for a user, we first query to see last time the feed was generated. New feed will be generated from that time onwards.\n",
    "\n",
    "We can store this data in a hash table where key = UserID and value is:\n",
    "```c\n",
    "Struct { \n",
    "    LinkedHashMap <PostID, Post> posts;\n",
    "    DateTime lastGenerated;\n",
    "}\n",
    "```\n",
    "\n",
    "We can store the PostIDs in a Linked HashMap (A hash table + doubly-linked list implementation), which will allow for jumping to any post at constant time but also iterate through the map easily. (The linked list maintains the order in which keys were inserted into the map)\n",
    "\n",
    "When fetching new posts, the client sends the last PostID the user currently sees in their feed, then the server can jump to that PostID in our hashmap and return next batch of posts from there.\n",
    "\n",
    "#### How many feeds should we store in memory?\n",
    "Initially we can store 500 posts per user, but this number can be adjusted based on the usage pattern for a user. Users who never browse past 10 feeds can have 100 posts in memory.\n",
    "\n",
    "#### Should we generate for all users?\n",
    "No. Lots of users won't log in frequently.\n",
    "We can use a LRU based cache to remove users from memory that haven't accessed their feed for a long time.\n",
    "\n",
    "We can also use machine learning to pre-generate their feed based on their login patterns."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feed Publishing\n",
    "The process of pushing a post to all followers is call **fanout**.\n",
    "\n",
    "Two approaches to publishing:\n",
    "1. **Pull model (Fanout on load):** Clients pull data either on intervals or manually when needed. The problem with this approach is\n",
    "    \n",
    "    a. New data might not be shown to users until they do a pull request\n",
    "    \n",
    "    b. Most of the time pulls return empty responses: a wasted resource that could have been avoided.\n",
    "    \n",
    "2. **Push model (Fanout on write):** Immediately push a post to all followers once a user posts it. Advantage here is you don't need to iterate through your friends list to get their feeds, thus significantly reducing read operations. Users have to maintain a long-poll request with the server for receiving updates. A possible problem with this approach is when a celeb user has millions of followers, the server has to push updates to a lot of people.\n",
    "\n",
    "3. **Hybrid:** We can combine push and pull models. We stop pushing posts from celeb users with lots of followers. We can let their followers pull updates. By doing this, we can save a huge number of fanout resources. Alternatively, we can limit the push fanout to only followers who are online.\n",
    "\n",
    "#### How many feeds should we return to client?\n",
    "Say 20 per request. Also, different clients (mobile vs desktop) fetch different number of posts due to differences in screen size and bandwidth usage.\n",
    "\n",
    "We can notify the users on desktop where data usage is cheap. For mobile devices, data usage is expensive, so we can choose not to push data but instead to let users *pull to refresh* to get new posts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Feed Ranking\n",
    "To rank posts in a newsfeed, we can use creation time of the posts. However today's\n",
    "ranking algorithms are doing more to ensure important posts are ranked higher.\n",
    "\n",
    "The idea is to select key **features** that make a post important, \n",
    "combining them and calculating the final ranking score.\n",
    "\n",
    "These features include:\n",
    "* creation time\n",
    "* number of likes\n",
    "* number of comments\n",
    "* number of shares, \n",
    "* time of the updates\n",
    "\n",
    "We can also check the effectiveness of the ranking system by evaluating if it has increased user retention and add revenue etc. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Data Partitioning\n",
    "#### a. Sharding Posts and metadata\n",
    "Since we have a huge number of new posts every day and our read load is extremely high, we need to distribute data across multiple machines for better read/write efficiency. \n",
    "\n",
    "#### Sharding based on UserID\n",
    "We can try storing a user's data on one server. While storing:\n",
    "- Pass a UserID to our hash function that will map the user to a DB server where we'll store all of their data.\n",
    "- While querying for their data, we can ask the hash function where to find it and read it from there. \n",
    "\n",
    "Issues:\n",
    "1. What if a user is IG famous? There will be lots of queries on the server holding that user. This high load will affect the service's performance.\n",
    "2. Over time, some users will have more data compared to others. Maintaining a uniform distribution of growing data across servers is quite difficult. \n",
    "\n",
    "#### Sharding based on PostID\n",
    "A hash function maps each PostID to a random server where we can store that post.\n",
    "\n",
    "A centralized server running the offline feed generation service will:\n",
    "1. Find all the people the user follows.\n",
    "2. Send a query to all DB partitions to find posts from these people.\n",
    "3. Each DB server will find posts for each user, sort them by recency and return top posts to the centralized server.\n",
    "\n",
    "The service will merge all results and sort them again to be stored in cache; ready to be retrieved whenever a user does a pull request. This solves the problem of hot users.\n",
    "\n",
    "Issues with this approach:\n",
    "- We have to query all DB partitions to find posts for a user, leading to higher latencies.\n",
    "\n",
    "> We can improve the performance by caching hot posts in front of the DB servers.\n",
    "\n",
    "#### Sharding based on Post creation time\n",
    "\n",
    "Storing posts based on creation timestamp will help us to fetch all top posts quickly and we only have to query a very small set of database servers.\n",
    "\n",
    "Issues:\n",
    "- Traffic load won't be distributed. e.g when writing, all new posts will be going to one server, and remaining servers sit idle. When reading, server holding the latest data will have high load as compared to servers holding old data.\n",
    "\n",
    "#### Sharding by PostID + Post creation time\n",
    "Each PostID should be universally unique and contain a timestamp.\n",
    "We can use epoch time for this. \n",
    "\n",
    "![](images/twitter_epoch.png)\n",
    "\n",
    "First part of PostID will be a timestamp, second part an auto-incrementing number. We can then figure out the shard number from this PostId and store it there.\n",
    "\n",
    "For fault tolerance and better performance, we can have two DB servers to generate auto-incrementing keys; one odd numbers and one even numbered keys.\n",
    "\n",
    "If we assume our current epoch seconds begins now, PostIDs will look like this:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch - autoincrement\n",
      "1571691220-000001\n",
      "1571691220-000002\n",
      "1571691220-000003\n",
      "1571691220-000004\n"
     ]
    }
   ],
   "source": [
    "epoch = 1571691220\n",
    "print('epoch - autoincrement')\n",
    "for i in range(1,5):\n",
    "    print(f'{epoch}-{i:06}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
